%%%%%%%%%%%%%%%%%%%% MetalFish Paper %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% MetalFish: GPU-Accelerated Chess Engine on Apple Silicon
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{svproc}

\usepackage{url}
\def\UrlFont{\rmfamily}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% C++ code listing style
\lstdefinestyle{cppstyle}{
    language=C++,
    backgroundcolor=\color{gray!5},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=2,
    frame=single,
    keywordstyle=\color{blue!70!black},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red!60!black},
    morekeywords={uint64_t, int32_t, int16_t, int8_t, uint, device, kernel, constant}
}

\begin{document}
\mainmatter

\title{MetalFish: Practical Limits of GPU-Accelerated\\NNUE Evaluation on Apple Silicon}

\titlerunning{MetalFish: GPU NNUE on Apple Silicon}

\author{Nripesh Niketan\inst{1}}

\authorrunning{N. Niketan}

\institute{Independent Researcher\\
\email{nripesh14@gmail.com}}

\maketitle

\begin{abstract}
We present MetalFish, a GPU-accelerated chess engine exploring the practical limits of NNUE evaluation on Apple Silicon's unified memory architecture. Through systematic benchmarking on M2 Max, we demonstrate that synchronous GPU dispatch overhead (148~$\mu$s median) dominates single-position latency (253~$\mu$s), making GPU evaluation approximately 337$\times$ slower than CPU NNUE (0.75~$\mu$s at 1.34M NPS) for alpha-beta search. However, batch evaluation amortizes this overhead effectively: per-position cost drops to 0.3~$\mu$s at batch size 4096, with true single-dispatch batching achieving 567$\times$ speedup over sequential dispatches. We provide stage-by-stage latency decomposition showing GPU dispatch accounts for $>$99\% of single-position time, implement adaptive kernel selection with dual-perspective feature transformation using 8-way loop unrolling, and verify 100\% evaluation consistency across 1,000 positions. GPU acceleration remains promising for batch-oriented workloads (MCTS, database analysis) but is unsuitable for traditional alpha-beta search without speculative evaluation or asynchronous queuing.

\keywords{Chess Engine, GPU Computing, Metal, NNUE, Apple Silicon, Unified Memory}
\end{abstract}

\section{Introduction}

Modern chess engines combine alpha-beta search with neural network evaluation (NNUE) to achieve superhuman playing strength. The critical path in alpha-beta search is \textit{latency}, not throughput: each position must be evaluated before pruning decisions can be made. In contrast, Monte Carlo Tree Search (MCTS) is \textit{throughput}-oriented, naturally batching leaf evaluations.

Apple Silicon's unified memory architecture eliminates explicit CPU-GPU memory transfers, potentially reducing the overhead that has historically limited GPU adoption in alpha-beta engines. This paper presents MetalFish, a GPU-accelerated chess engine that systematically measures where time is spent in GPU NNUE evaluation, revealing that command buffer dispatch---not compute---dominates single-position latency.

\subsection{Research Question}

What are the practical limits of GPU-accelerated NNUE evaluation on Apple Silicon, and where exactly does the time go?

\subsection{Contributions}

\begin{enumerate}
\item \textbf{Quantified CPU vs GPU NNUE comparison}: CPU NNUE achieves 1.34M positions/second (0.75~$\mu$s/pos); GPU single-position blocking latency is 253~$\mu$s (337$\times$ slower).

\item \textbf{Stage-by-stage latency decomposition}: We show GPU dispatch and synchronization account for $>$99\% of single-position time, with CPU feature extraction negligible (0.04~$\mu$s).

\item \textbf{Batch scaling analysis}: Per-position cost drops from 253~$\mu$s (N=1) to 0.3~$\mu$s (N=4096), with dispatch overhead amortized across positions.

\item \textbf{True batching verification}: Single command buffer with two dispatches achieves 567$\times$ speedup over N sequential command buffers at N=1024.

\item \textbf{GPU evaluation consistency}: 100\% reproducibility across 1,000 positions with non-zero scores.
\end{enumerate}

\section{Background}

\subsection{NNUE Architecture}

Stockfish's NNUE~\cite{Stockfish2024,Nasu2018} uses HalfKAv2\_hm features with sparse input and efficient incremental updates. Table~\ref{tab:nnue_arch} summarizes the architecture.

\begin{table}[t]
\caption{NNUE Network Architecture}
\label{tab:nnue_arch}
\centering
\begin{tabular}{lrr}
\toprule
Component & Big Network & Small Network \\
\midrule
Feature set & HalfKAv2\_hm & HalfKAv2\_hm \\
Input features & 45,056 & 22,528 \\
Hidden dimension & 1,024 & 128 \\
FC0 output & 15 (+1 skip) & 15 (+1 skip) \\
FC1 output & 32 & 32 \\
FC2 output & 1 & 1 \\
Layer stacks (buckets) & 8 & 8 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Feature requirements}: HalfKAv2\_hm generates one feature per non-king piece from each perspective. We support 64 features per perspective; in our 2,048-position dataset, maximum observed was 30 per perspective (60 total).

\subsection{Alpha-Beta vs MCTS}

Alpha-beta search~\cite{Knuth1975} evaluates positions sequentially with data-dependent pruning. Each evaluation must complete before the next pruning decision. This makes \textit{latency} the critical metric.

MCTS~\cite{Silver2017} accumulates positions at leaf nodes before evaluation, naturally forming batches. This makes \textit{throughput} the critical metric, where GPU acceleration excels.

\subsection{Metal Compute Model}

Apple Metal~\cite{AppleMetal2024} provides GPU compute through command buffers with unified memory access:
\begin{itemize}
\item \textbf{Unified memory}: CPU and GPU share physical memory, eliminating explicit transfers
\item \textbf{Command buffer lifecycle}: Allocation $\rightarrow$ encoding $\rightarrow$ commit $\rightarrow$ waitUntilCompleted
\item \textbf{Dispatch overhead}: Each command buffer submission incurs fixed overhead regardless of kernel complexity
\end{itemize}

\section{System Architecture}

\subsection{GPU Configuration}

Table~\ref{tab:gpu_constants} shows the key GPU configuration parameters.

\begin{table}[t]
\caption{GPU Configuration Constants}
\label{tab:gpu_constants}
\centering
\begin{tabular}{lr}
\toprule
Parameter & Value \\
\midrule
Max batch size & 4,096 \\
Max features per perspective & 64 \\
Threadgroup size & 256 \\
SIMD group size & 32 \\
Forward pass threads & 64 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Adaptive Kernel Selection}

We implement strategy-based kernel selection:

\begin{lstlisting}[style=cppstyle,caption={Evaluation strategy selection}]
enum class EvalStrategy {
  CPU_FALLBACK,   // batch < 4
  GPU_STANDARD,   // batch < 64
  GPU_SIMD        // batch >= 64
};
\end{lstlisting}

For batches $\geq 64$, we use dual-perspective kernels that process both white and black perspectives in a single 3D dispatch. The feature transform kernel uses 8-way loop unrolling for maximum instruction-level parallelism, with feature index bounds checking removed since CPU extraction guarantees valid indices.

\subsection{Zero-Copy Buffer Management}

We pre-allocate all working buffers at initialization, writing features directly to unified memory:

\begin{lstlisting}[style=cppstyle,caption={Direct buffer writes}]
// Write directly to unified memory (zero-copy)
int32_t* features_ptr = 
    static_cast<int32_t*>(features_buffer_->data());
for (int i = 0; i < batch_size; i++) {
  features_ptr[offset++] = feature_index;
}
\end{lstlisting}

\subsection{Batch Evaluation Pipeline}

Algorithm~\ref{alg:batch} shows the evaluation pipeline.

\begin{algorithm}[t]
\caption{GPU Batch NNUE Evaluation}
\label{alg:batch}
\begin{algorithmic}[1]
\Require Batch of $N$ positions
\Ensure Evaluation scores for all positions
\State $strategy \gets$ \Call{SelectStrategy}{$N$}
\If{$strategy = $ CPU\_FALLBACK}
    \State \Return CPU evaluation
\EndIf
\State \textbf{// CPU: Extract features to unified memory}
\For{each position}
    \State Write features directly to GPU buffers
\EndFor
\State \textbf{// GPU: Single command buffer}
\State \Call{DispatchThreads}{$hidden\_dim, 2, N$} \Comment{Feature transform}
\State \Call{Barrier}{}
\State \Call{DispatchThreadgroups}{$N$, threads=64} \Comment{Forward pass}
\State \Call{SubmitAndWait}{} \Comment{Blocking sync}
\State \Return scores from output buffer
\end{algorithmic}
\end{algorithm}

\section{Experimental Methodology}

\subsection{Hardware and Software}

\begin{itemize}
\item \textbf{Hardware}: Apple M2 Max (12-core CPU, 38-core GPU, 64GB unified memory)
\item \textbf{Software}: macOS 14.0, Xcode 15.0, Metal 3.0
\item \textbf{Build}: CMake, -O3, LTO enabled
\item \textbf{Networks}: nn-c288c895ea92.nnue (125MB big), nn-37f18f62d772.nnue (6MB small)
\end{itemize}

\subsection{Benchmark Dataset}

Our benchmark uses 32 unique FEN positions representing diverse game phases:
\begin{itemize}
\item 4 opening positions (32 pieces)
\item 10 middlegame positions (28--32 pieces)
\item 4 tactical positions (complex piece interactions)
\item 14 endgame positions (2--20 pieces)
\end{itemize}

These are cycled to create 2,048 test positions. Of these, 1,984 positions have no king in check (valid for NNUE evaluation).

\subsection{Timing Methodology}

\begin{itemize}
\item \textbf{Timer}: \texttt{std::chrono::high\_resolution\_clock}
\item \textbf{Warmup}: 100 iterations discarded
\item \textbf{Samples}: 100--100,000 iterations depending on variance
\item \textbf{Statistics}: Median, P95, P99 reported
\item \textbf{GPU timing}: Blocking \texttt{waitUntilCompleted()} (synchronous)
\end{itemize}

\subsection{CPU NNUE Baseline}

CPU NNUE performance was measured using the engine's standard \texttt{bench} command, which runs depth-limited searches on 50 diverse positions:
\begin{itemize}
\item \textbf{Nodes searched}: 2,477,446
\item \textbf{Total time}: 1,846 ms
\item \textbf{NPS}: 1,342,061 nodes/second
\item \textbf{Per-position latency}: $\approx$0.75~$\mu$s
\end{itemize}

This represents full NNUE evaluation including accumulator updates and forward pass, providing a matched-scope baseline for GPU comparison.

\section{Results}

\subsection{CPU Baseline Measurements}

Table~\ref{tab:cpu_baseline} shows CPU evaluation performance.

\begin{table}[t]
\caption{CPU Evaluation Baselines}
\label{tab:cpu_baseline}
\centering
\begin{tabular}{lrr}
\toprule
Metric & Simple Eval & NNUE (bench) \\
\midrule
Median latency & 0.00~$\mu$s & 0.75~$\mu$s \\
Throughput & $>$10M/s & 1.34M/s \\
\bottomrule
\end{tabular}
\end{table}

CPU NNUE evaluation achieves 1.34 million positions per second, or approximately 0.75~$\mu$s per position. This is the baseline against which GPU evaluation must be compared.

\subsection{GPU Dispatch Overhead}

Table~\ref{tab:dispatch} shows minimal-kernel dispatch overhead.

\begin{table}[t]
\caption{GPU Dispatch Overhead---Minimal Kernel (N=1,000)}
\label{tab:dispatch}
\centering
\begin{tabular}{lr}
\toprule
Statistic & Latency ($\mu$s) \\
\midrule
Median & 148 \\
P95 & 260 \\
P99 & 316 \\
\bottomrule
\end{tabular}
\end{table}

The 148~$\mu$s median dispatch overhead represents the \textit{irreducible minimum} cost for any GPU operation in synchronous blocking mode. This alone is 197$\times$ slower than CPU NNUE evaluation.

\subsection{GPU Stage Breakdown}

Table~\ref{tab:stage_breakdown} decomposes end-to-end GPU latency into stages.

\begin{table}[t]
\caption{GPU Stage Breakdown (median, N=100 iterations)}
\label{tab:stage_breakdown}
\centering
\begin{tabular}{lrrrr}
\toprule
Batch & CPU Prep & GPU Eval & Total & GPU \% \\
Size & ($\mu$s) & ($\mu$s) & ($\mu$s) & \\
\midrule
1 & 0.3 & 258 & 258 & 99.9\% \\
8 & 0.3 & 258 & 258 & 99.9\% \\
512 & 5.8 & 362 & 368 & 98.4\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key finding}: GPU dispatch and synchronization dominate ($>$98\% of total time). CPU feature extraction is negligible due to zero-copy buffer management.

\subsection{Batch Latency Scaling}

Table~\ref{tab:batch_latency} shows end-to-end latency across batch sizes.

\begin{table}[t]
\caption{GPU End-to-End Batch Latency (N=100 iterations)}
\label{tab:batch_latency}
\centering
\begin{tabular}{rrrrr}
\toprule
Batch & Median & P95 & P99 & Per-Pos \\
Size & ($\mu$s) & ($\mu$s) & ($\mu$s) & ($\mu$s) \\
\midrule
1 & 253 & 363 & 395 & 253.0 \\
8 & 247 & 347 & 470 & 30.8 \\
64 & 279 & 538 & 662 & 4.4 \\
256 & 341 & 446 & 497 & 1.3 \\
512 & 347 & 462 & 485 & 0.7 \\
1024 & 500 & 596 & 626 & 0.5 \\
2048 & 824 & 937 & 960 & 0.4 \\
4096 & 1,359 & 1,467 & 1,607 & 0.3 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings}:
\begin{enumerate}
\item Latency is approximately constant (247--341~$\mu$s) for batch sizes 1--256, confirming dispatch dominance.
\item Per-position cost drops from 253~$\mu$s (N=1) to 0.3~$\mu$s (N=4096).
\item GPU becomes throughput-competitive with CPU NNUE (0.75~$\mu$s) only at batch sizes $\geq$512.
\end{enumerate}

\subsection{True Batching Verification}

Table~\ref{tab:batching} compares sequential vs batched dispatches.

\begin{table}[t]
\caption{True Batching Verification (N=50 iterations)}
\label{tab:batching}
\centering
\begin{tabular}{rrrr}
\toprule
N & Sequential & Batched & Speedup \\
  & (N$\times$1 CB) & (1$\times$1 CB) & \\
\midrule
16 & 4,708~$\mu$s & 283~$\mu$s & 16.6$\times$ \\
64 & 18,573~$\mu$s & 291~$\mu$s & 63.8$\times$ \\
256 & 73,986~$\mu$s & 300~$\mu$s & 246.3$\times$ \\
1024 & 292,860~$\mu$s & 516~$\mu$s & 567.2$\times$ \\
\bottomrule
\end{tabular}
\end{table}

Speedups scale approximately linearly with batch size because each sequential dispatch incurs the full 146~$\mu$s overhead, while batching amortizes this cost.

\subsection{GPU Evaluation Consistency}

Table~\ref{tab:correctness} verifies GPU evaluation reproducibility.

\begin{table}[t]
\caption{GPU Evaluation Consistency (1,000 positions)}
\label{tab:correctness}
\centering
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Non-zero GPU scores & 100\% \\
Consistent across runs & 100\% \\
Mean $|$score$|$ & 222 \\
Score range & [-407, 258] \\
\bottomrule
\end{tabular}
\end{table}

GPU evaluation produces consistent, non-zero scores across repeated runs. The score range indicates meaningful differentiation between positions.

\section{Discussion}

\subsection{Why GPU is Slower for Single Positions}

The fundamental bottleneck is command buffer dispatch overhead, not compute:
\begin{itemize}
\item Minimal kernel dispatch: 148~$\mu$s
\item NNUE kernel dispatch: 253~$\mu$s
\item CPU NNUE evaluation: 0.75~$\mu$s
\end{itemize}

Even with zero kernel execution time, GPU would still be 197$\times$ slower than CPU for single positions due to dispatch overhead alone.

\subsection{When GPU Evaluation Helps}

GPU batch evaluation becomes throughput-competitive at $N \geq 512$ (0.7~$\mu$s/position vs CPU's 0.75~$\mu$s). This enables:
\begin{itemize}
\item \textbf{MCTS evaluation}: Monte Carlo Tree Search naturally batches leaf evaluations
\item \textbf{Database analysis}: Evaluating thousands of positions from game databases
\item \textbf{Training data generation}: Bulk position evaluation for neural network training
\end{itemize}

\subsection{Alpha-Beta Limitations}

Single-position GPU blocking latency (253~$\mu$s) makes GPU evaluation unsuitable for alpha-beta search in synchronous blocking mode. Alpha-beta's sequential, data-dependent pruning prevents effective batching without significant architectural changes such as:
\begin{itemize}
\item Speculative evaluation of multiple branches
\item Asynchronous queuing with CPU fallback
\item Lazy evaluation with deferred GPU dispatch
\end{itemize}

Our asynchronous evaluation API (\texttt{evaluate\_batch\_async}) enables CPU/GPU overlap, but the fundamental sequential nature of alpha-beta limits its applicability.

\subsection{Threats to Validity}

\begin{itemize}
\item \textbf{Synchronous timing}: Our measurements use blocking \texttt{waitUntilCompleted()}. Asynchronous dispatch with completion handlers could reduce apparent latency by overlapping CPU work.
\item \textbf{Command buffer reuse}: We create new command buffers per evaluation. Reusing command buffers could reduce allocation overhead.
\item \textbf{Dataset size}: Our 32 unique positions may not represent all game phases equally.
\end{itemize}

\section{Related Work}

Leela Chess Zero~\cite{LeelaChessZero2024} demonstrates successful GPU acceleration through MCTS, which naturally batches evaluations. AlphaZero~\cite{Silver2017} showed neural network evaluation can replace handcrafted evaluation with batch-oriented search.

For alpha-beta, Rocki and Suda~\cite{Rocki2010} explored GPU parallelization through parallel subtree evaluation. Our work extends this to unified memory hardware with quantified bottleneck analysis.

Apple's Metal documentation~\cite{AppleMetal2024,AppleMetalBestPractices2024} recommends minimizing command buffer submissions and using threadgroup memory for intermediate results.

\section{Conclusion}

We presented MetalFish, a GPU-accelerated chess engine that quantifies the practical limits of GPU NNUE evaluation on Apple Silicon:

\begin{enumerate}
\item \textbf{CPU NNUE baseline}: 1.34M positions/second (0.75~$\mu$s/pos)
\item \textbf{GPU single-position}: 253~$\mu$s median (337$\times$ slower than CPU)
\item \textbf{GPU dispatch overhead}: 148~$\mu$s irreducible minimum
\item \textbf{GPU batch (N=4096)}: 0.3~$\mu$s/pos (throughput-competitive)
\item \textbf{True batching}: 567$\times$ speedup at N=1024
\item \textbf{Consistency}: 100\% reproducibility across 1,000 positions
\end{enumerate}

\textbf{Key insight}: GPU dispatch overhead, not compute, is the bottleneck. Single-position GPU evaluation is unsuitable for alpha-beta search, but batch evaluation is effective for MCTS, database analysis, and training data generation.

\subsection*{Reproducibility}

\textbf{Hardware}: Apple M2 Max, 64GB. \textbf{Software}: macOS 14.0, Xcode 15.0. \textbf{Build}: CMake, -O3, LTO. \textbf{Source}: \url{https://github.com/NripeshN/MetalFish}. \textbf{Benchmark}: \texttt{gpubench} UCI command.

\begin{thebibliography}{10}

\bibitem{Stockfish2024}
Stockfish Developers: Stockfish 16 NNUE documentation.
\url{https://github.com/official-stockfish/Stockfish} (2024)

\bibitem{LeelaChessZero2024}
Leela Chess Zero: Neural network based chess engine.
\url{https://lczero.org/} (2024)

\bibitem{Silver2017}
Silver, D., et al.: Mastering chess and shogi by self-play with a general reinforcement learning algorithm.
arXiv:1712.01815 (2017)

\bibitem{Rocki2010}
Rocki, K., Suda, R.: Parallel minimax tree searching on GPU.
In: Parallel Processing and Applied Mathematics, LNCS vol. 6067, pp. 449--456. Springer (2010)

\bibitem{Nasu2018}
Nasu, Y.: Efficiently updatable neural-network-based evaluation functions for computer shogi.
The 28th World Computer Shogi Championship Appeal Document (2018)

\bibitem{AppleMetal2024}
Apple Inc.: Metal Programming Guide.
\url{https://developer.apple.com/metal/} (2024)

\bibitem{AppleMetalBestPractices2024}
Apple Inc.: Metal Best Practices Guide.
\url{https://developer.apple.com/library/archive/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/} (2024)

\bibitem{Knuth1975}
Knuth, D.E., Moore, R.W.: An analysis of alpha-beta pruning.
Artificial Intelligence 6(4), 293--326 (1975)

\end{thebibliography}

\end{document}
