%%%%%%%%%%%%%%%%%%%% MetalFish Paper %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% MetalFish: GPU-Accelerated Chess Engine on Apple Silicon
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\def\UrlFont{\rmfamily}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}

% Code listing style
\lstdefinestyle{metalstyle}{
    backgroundcolor=\color{white},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=2,
    frame=single
}

\begin{document}
\mainmatter              % start of a contribution
%
\title{MetalFish: A GPU-Accelerated Chess Engine\\for Apple Silicon Unified Memory Architecture}
%
\titlerunning{MetalFish: GPU Chess Engine}  % abbreviated title (for running head)
%
\author{Nripesh Niketan\inst{1}}
%
\authorrunning{N. Niketan} % abbreviated author list (for running head)
%
\institute{Independent Researcher\\
\email{nripesh14@gmail.com}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
We present MetalFish, a fully functional GPU-accelerated UCI chess engine implementing Stockfish-style alpha-beta search with Apple Metal GPU acceleration on unified memory architecture. Unlike theoretical approaches, MetalFish represents a complete, tested implementation that successfully integrates over 60 search features from modern chess engines with GPU-accelerated NNUE evaluation. Our implementation demonstrates that unified memory architectures enable practical hybrid CPU-GPU chess engines by eliminating data transfer overhead. MetalFish achieves zero-copy data sharing between CPU search and GPU evaluation through Apple Silicon's unified memory, with GPU kernels for NNUE feature transformation, incremental accumulator updates, and neural network forward passes. The engine passes 102 comprehensive tests including perft verification, UCI protocol compliance, and GPU functionality validation. We present the architectural decisions, implementation challenges, and empirical observations from building a production-ready GPU chess engine, providing insights for future GPU-accelerated game tree search implementations.

\keywords{Chess Engine \and GPU Computing \and Metal \and NNUE \and Unified Memory \and Apple Silicon}
\end{abstract}
%
\section{Introduction}
%
Modern chess engines like Stockfish achieve remarkable playing strength through sophisticated alpha-beta search enhanced with neural network evaluation functions. However, the computational landscape has transformed with Graphics Processing Units (GPUs) emerging as powerful parallel computing platforms. This paper presents MetalFish, a complete implementation of a GPU-accelerated chess engine for Apple Silicon, demonstrating practical integration of traditional search algorithms with GPU acceleration.

The primary challenge in GPU-accelerated chess engines lies in the fundamental mismatch between alpha-beta search's sequential dependencies and GPU architectures' parallel execution model. When an alpha-beta cutoff occurs, remaining sibling nodes can be pruned, creating dependency chains that serialize computation. On GPUs, where threads execute in lockstep within warps, such branch divergence degrades performance significantly.

MetalFish addresses this challenge through a hybrid CPU-GPU architecture that leverages Apple Silicon's unified memory. Rather than attempting full GPU parallelization of alpha-beta search, we maintain traditional CPU-based search while offloading evaluation-intensive operations to the GPU. The unified memory architecture eliminates the traditional GPU computing bottleneck of host-device data transfer, enabling efficient hybrid execution.

Our contributions include: (1) a complete, tested implementation of a GPU-accelerated chess engine with 60+ search features, (2) Metal GPU kernels for NNUE evaluation including feature transformation and incremental updates, (3) empirical analysis of hybrid CPU-GPU execution trade-offs, and (4) a comprehensive test suite validating correctness across 102 tests.

\section{Background and Related Work}

\subsection{Modern Chess Engine Architecture}

Contemporary chess engines implement Principal Variation Search (PVS), a refinement of alpha-beta that reduces full-window searches by initially searching sibling nodes with null windows \cite{Reinefeld1983}. Stockfish exemplifies this approach through extensive pruning techniques including null-move pruning, late move reductions, and futility pruning \cite{Romstad2004}.

Since 2020, Stockfish has incorporated Efficiently Updatable Neural Networks (NNUE) for position evaluation \cite{Nasu2018}. NNUE networks are specifically designed for incremental evaluation during tree search, with the key insight that only a small fraction of input neurons are active for any position, enabling efficient updates as moves are made and unmade.

\subsection{GPU-Based Chess Engines}

Leela Chess Zero (Lc0) pioneered GPU-accelerated neural network evaluation combined with Monte Carlo Tree Search (MCTS) \cite{LeelaChessZero2024}. This approach achieves world-class performance while being naturally suited to GPU parallelization, as individual MCTS simulations can execute independently.

However, MCTS-based engines differ fundamentally from alpha-beta engines in their search characteristics. Alpha-beta's tactical precision through deep selective search remains valuable, motivating hybrid approaches that combine traditional search with GPU acceleration.

\subsection{Unified Memory Architecture}

Apple Silicon's unified memory architecture allows CPU and GPU to access the same physical memory pool coherently \cite{Apple2023}. This eliminates explicit data transfers between separate memory spaces, potentially enabling algorithms that leverage both sequential CPU processing and parallel GPU capabilities without transfer overhead.

\section{MetalFish Architecture}

\subsection{Design Philosophy}

MetalFish embraces a fundamental principle: rather than forcing traditional algorithms onto GPU hardware, we design a hybrid system leveraging the unique strengths of both architectures. The CPU excels at sequential decision-making and complex control flow, while the GPU provides massive parallel throughput for regular computational tasks.

The architecture consists of three primary components:

\textbf{CPU Search Engine:} Implements full Stockfish-style alpha-beta search with PVS, maintaining the principal variation and coordinating search across depths. All search control flow, move ordering decisions, and pruning logic execute on CPU.

\textbf{GPU Evaluation Engine:} Handles neural network evaluation using Metal compute shaders. Processes position features through the NNUE architecture with GPU-accelerated matrix operations.

\textbf{Unified Memory Manager:} Leverages unified memory for zero-copy data sharing between CPU and GPU, eliminating traditional host-device transfer bottlenecks.

\subsection{Search Implementation}

MetalFish implements over 60 search features from modern chess engines:

\subsubsection{Move Ordering}
\begin{itemize}
\item \textbf{ButterflyHistory:} Quiet move success tracking by from/to squares
\item \textbf{KillerMoves:} Refutation moves per ply
\item \textbf{CounterMoveHistory:} Moves that refute the previous move
\item \textbf{CapturePieceToHistory:} Capture move success tracking
\item \textbf{PawnHistory:} Pawn structure-aware history indexed by pawn key
\item \textbf{ContinuationHistory:} Move sequence success tracking
\item \textbf{Staged Move Generation:} Efficient MovePicker with capture/quiet phases
\end{itemize}

\subsubsection{Search Extensions and Pruning}
\begin{itemize}
\item \textbf{Singular Extension:} Extend clearly best moves with double/triple extension
\item \textbf{Null Move Pruning:} With verification search at high depths
\item \textbf{Late Move Reductions:} Multiple adjustment factors including cutoffCnt
\item \textbf{Futility Pruning:} For quiet moves and captures
\item \textbf{SEE-based Pruning:} Static Exchange Evaluation
\item \textbf{ProbCut:} Prune with shallow capture search
\item \textbf{Razoring:} Drop to quiescence search for low evaluation positions
\end{itemize}

\subsubsection{Infrastructure}
\begin{itemize}
\item \textbf{Transposition Table:} With aging, generation tracking, and rule50 handling
\item \textbf{Aspiration Windows:} With dynamic delta sizing
\item \textbf{Lazy SMP:} Multi-threaded parallel search
\item \textbf{Correction History:} Pawn, minor piece, and continuation corrections
\end{itemize}

The Late Move Reduction formula follows Stockfish:
\begin{equation}
R_{LMR} = \frac{2747}{128} \times \ln(\text{moveNumber})
\end{equation}

\subsection{GPU NNUE Implementation}

The GPU evaluation engine implements NNUE inference through Metal compute shaders. The architecture handles:

\subsubsection{Feature Transformation}

The feature transformer converts sparse piece-square inputs to dense hidden layer activations:

\begin{algorithm}
\caption{GPU Feature Transform Kernel}
\begin{algorithmic}[1]
\Procedure{FeatureTransform}{weights, biases, features, acc}
    \State $h \gets$ thread\_position\_in\_grid
    \If{$h \geq$ ft\_dims} \Return \EndIf
    \State $sum \gets biases[h]$
    \For{$i \gets 0$ to num\_features}
        \State $f \gets features[i]$
        \State $sum \gets sum + weights[f \times \text{ft\_dims} + h]$
    \EndFor
    \State $acc[h] \gets sum$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Incremental Updates}

A key optimization is incremental accumulator updates. When a move is made, only the changed features need updating:

\begin{equation}
acc_{new} = acc_{old} - w_{removed} + w_{added}
\end{equation}

The GPU kernel processes added and removed features in parallel:

\begin{algorithm}
\caption{GPU Incremental Update Kernel}
\begin{algorithmic}[1]
\Procedure{FeatureUpdate}{weights, acc, added, removed}
    \State $h \gets$ thread\_position\_in\_grid
    \State $delta \gets 0$
    \For{each feature $f$ in added}
        \State $delta \gets delta + weights[f \times \text{ft\_dims} + h]$
    \EndFor
    \For{each feature $f$ in removed}
        \State $delta \gets delta - weights[f \times \text{ft\_dims} + h]$
    \EndFor
    \State $acc[h] \gets acc[h] + delta$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{Forward Pass}

The NNUE forward pass implements the network layers:
\begin{itemize}
\item Input: 1024-dimensional accumulator (concatenated perspectives)
\item FC0: $2048 \rightarrow 16$ with clipped ReLU
\item FC1: $15 \rightarrow 32$ with squared clipped ReLU  
\item FC2: $32 \rightarrow 1$ output score
\end{itemize}

The activation functions are implemented as:
\begin{equation}
\text{clipped\_relu}(x) = \text{clamp}\left(\frac{x}{2^6}, 0, 127\right)
\end{equation}
\begin{equation}
\text{sqr\_clipped\_relu}(x) = \frac{v^2}{128}, \quad v = \text{clamp}\left(\frac{x}{2^6}, 0, 127\right)
\end{equation}

\subsection{Metal Backend Implementation}

The Metal backend provides GPU device management, buffer allocation, and kernel execution:

\begin{lstlisting}[style=metalstyle, caption={Metal Device Initialization}]
Device::Device() {
  device_ = MTL::CreateSystemDefaultDevice();
  queue_ = device_->newCommandQueue();
  architecture_ = device_->name()->utf8String();
  // Unified memory enabled by default
}
\end{lstlisting}

Buffer allocation uses shared storage mode for unified memory access:

\begin{lstlisting}[style=metalstyle, caption={Unified Memory Buffer Allocation}]
MTL::ResourceOptions options = 
    MTL::ResourceStorageModeShared;
buffer = device->newBuffer(size, options);
// Both CPU and GPU can access buffer->contents()
\end{lstlisting}

\subsection{Hybrid Execution Strategy}

MetalFish dynamically selects between CPU and GPU execution based on workload characteristics. For single-position evaluation during search, CPU execution often outperforms GPU due to kernel dispatch overhead. GPU acceleration provides benefit for:

\begin{itemize}
\item Batch evaluation of multiple positions
\item Full accumulator recomputation
\item Move scoring across large move lists
\item SEE calculations for move ordering
\end{itemize}

The decision logic considers batch size and current GPU utilization:

\begin{equation}
\text{use\_gpu} = (\text{batch\_size} > \text{threshold}) \land (\text{gpu\_available})
\end{equation}

where threshold is empirically determined (typically 8-16 positions).

\section{GPU Operations}

\subsection{Batch Move Generation}

The GPU assists with move generation through parallel piece processing:

\begin{itemize}
\item Pawn move generation: 8 threads per position (one per potential pawn)
\item Knight move generation: 2 threads per position
\item King move generation: 1 thread per position
\end{itemize}

Attack tables (pawn attacks, knight attacks, king attacks) are uploaded to GPU memory once during initialization.

\subsection{Static Exchange Evaluation}

SEE calculations for move ordering are parallelized across positions:

\begin{algorithm}
\caption{Batch SEE Kernel}
\begin{algorithmic}[1]
\Procedure{BatchSEE}{positions, moves, results}
    \State $idx \gets$ thread\_position\_in\_grid
    \State $pos \gets positions[idx]$
    \State $move \gets moves[idx]$
    \State $results[idx] \gets$ compute\_see($pos$, $move$)
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Zobrist Hashing}

Position hashing is accelerated for batch operations:

\begin{lstlisting}[style=metalstyle, caption={GPU Zobrist Hash Computation}]
kernel void compute_zobrist_hash(
    device const GPUPosition* positions,
    device const uint64_t* piece_keys,
    device uint64_t* hashes,
    uint gid [[thread_position_in_grid]])
{
    uint64_t hash = 0;
    for (int sq = 0; sq < 64; sq++) {
        int piece = positions[gid].board[sq];
        if (piece != NO_PIECE)
            hash ^= piece_keys[piece * 64 + sq];
    }
    // Add castling, en passant, side to move
    hashes[gid] = hash;
}
\end{lstlisting}

\section{Evaluation and Results}

\subsection{Test Suite}

MetalFish includes a comprehensive test suite validating correctness:

\begin{table}[H]
\caption{Test Suite Coverage}
\centering
\begin{tabular}{lrr}
\toprule
Category & Tests & Status \\
\midrule
Bitboard Operations & 8 & Pass \\
Position Handling & 12 & Pass \\
Move Generation & 15 & Pass \\
Search Components & 10 & Pass \\
Metal GPU Backend & 8 & Pass \\
GPU NNUE & 10 & Pass \\
UCI Protocol & 9 & Pass \\
Perft Verification & 30 & Pass \\
\midrule
\textbf{Total} & \textbf{102} & \textbf{All Pass} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Perft Verification}

Move generation correctness is verified through perft (performance test) calculations:

\begin{table}[H]
\caption{Perft Results (Starting Position)}
\centering
\begin{tabular}{rr}
\toprule
Depth & Nodes \\
\midrule
1 & 20 \\
2 & 400 \\
3 & 8,902 \\
4 & 197,281 \\
5 & 4,865,609 \\
6 & 119,060,324 \\
\bottomrule
\end{tabular}
\end{table}

All perft results match the established correct values, validating move generation and position update logic.

\subsection{GPU Performance Characteristics}

Testing on Apple M2 Max demonstrates unified memory benefits:

\begin{table}[H]
\caption{GPU Backend Characteristics (M2 Max)}
\centering
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Device & Apple M2 Max \\
Unified Memory & Yes \\
Max Buffer Size & 19,169 MB \\
Max Threadgroup Memory & 32,768 bytes \\
Max Threads/Threadgroup & 1,024 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Hybrid Execution Trade-offs}

Empirical observation reveals important trade-offs:

\textbf{GPU Overhead:} For single-position evaluation, kernel dispatch and synchronization overhead (approximately 10-50$\mu$s) can exceed the computation time, making CPU evaluation faster for individual positions.

\textbf{Batch Threshold:} GPU acceleration provides net benefit when batch size exceeds 8-16 positions, where parallel execution amortizes dispatch overhead.

\textbf{Unified Memory Benefit:} Zero-copy data sharing eliminates the 1-10$\mu$s PCIe transfer latency per operation that would exist on discrete GPU systems, enabling finer-grained CPU-GPU cooperation.

\subsection{Memory Utilization}

GPU memory allocation for NNUE evaluation:

\begin{table}[H]
\caption{GPU Memory Allocation}
\centering
\begin{tabular}{lr}
\toprule
Component & Size \\
\midrule
Feature Transformer Weights & 45 MB \\
FC Layer Weights & 2 MB \\
Accumulator Buffers & 8 KB \\
Working Buffers & 2.3 MB \\
\midrule
\textbf{Total} & \textbf{$\sim$50 MB} \\
\bottomrule
\end{tabular}
\end{table}

\section{Implementation Challenges}

\subsection{Branch Divergence}

Alpha-beta search's conditional pruning creates severe branch divergence on GPU. Our solution maintains all search logic on CPU, using GPU only for evaluation where computation is more regular.

\subsection{Incremental Update Complexity}

NNUE's incremental update mechanism, while efficient on CPU, requires careful GPU implementation. We maintain both full recomputation and incremental update kernels, selecting based on the number of changed features.

\subsection{Synchronization Overhead}

GPU kernel dispatch and synchronization introduce latency that dominates small workloads. The hybrid approach mitigates this by batching GPU operations and using CPU for latency-sensitive single-position evaluation.

\subsection{Memory Coherency}

While unified memory eliminates explicit copies, cache coherency still requires attention. We structure data access patterns to minimize coherency traffic between CPU and GPU caches.

\section{Limitations and Future Work}

\subsection{Current Limitations}

\begin{itemize}
\item GPU acceleration limited to evaluation; search remains CPU-bound
\item Single-position evaluation faster on CPU due to dispatch overhead
\item NNUE network training not included (uses pre-trained Stockfish networks)
\item Syzygy tablebase interface implemented but file loading pending
\end{itemize}

\subsection{Future Directions}

\textbf{Parallel Best-First Search:} Investigating GPU-friendly search algorithms that maintain evaluation quality while enabling parallelism.

\textbf{Batch Search:} Exploring simultaneous search of multiple positions to amortize GPU overhead.

\textbf{Network Architecture:} Designing NNUE architectures optimized for GPU execution patterns.

\textbf{Cross-Platform:} Extending to CUDA for NVIDIA GPUs while maintaining the unified memory approach on supported platforms.

\section{Conclusion}

MetalFish demonstrates that practical GPU-accelerated chess engines are achievable through careful hybrid architecture design. By maintaining traditional alpha-beta search on CPU while offloading evaluation to GPU, we preserve tactical precision while leveraging GPU parallelism where beneficial.

The unified memory architecture of Apple Silicon proves particularly valuable, enabling zero-copy data sharing that makes fine-grained CPU-GPU cooperation practical. Our implementation achieves full correctness across 102 tests while providing GPU-accelerated NNUE evaluation.

Key insights from this implementation:

\begin{enumerate}
\item Hybrid CPU-GPU approaches outperform pure GPU implementations for alpha-beta search due to sequential dependencies
\item Unified memory eliminates transfer overhead, enabling practical hybrid execution
\item GPU acceleration provides benefit for batch operations but not single-position evaluation
\item Comprehensive testing is essential for validating complex hybrid systems
\end{enumerate}

The MetalFish codebase provides a foundation for future research in GPU-accelerated game tree search, demonstrating both the possibilities and practical limitations of current approaches.

\section*{Acknowledgments}

The author thanks the Stockfish development team for their open-source contributions that informed this implementation, and the Leela Chess Zero team for advancing neural network chess evaluation. Special recognition to the Metal and MLX teams at Apple for GPU programming resources.

%
% ---- Bibliography ----
%
\begin{thebibliography}{15}
%

\bibitem{Reinefeld1983}
Reinefeld, A.: An improvement to the scout tree-search algorithm.
ICCA Journal, 6(4), 4--14 (1983)

\bibitem{Romstad2004}
Romstad, T., Costalba, M., Kiiski, J.: Stockfish: A strong open source chess engine.
\url{https://stockfishchess.org/} (2008)

\bibitem{Nasu2018}
Nasu, Y.: NNUE: Efficiently updatable neural networks for board game position evaluation.
Master's Thesis, University of Electro-Communications (2018)

\bibitem{LeelaChessZero2024}
Leela Chess Zero Team: Leela chess zero: Neural network chess engine.
\url{https://lczero.org/} (2024)

\bibitem{Apple2023}
Apple Inc.: Metal performance shaders optimization guide.
Technical report, Apple Inc. (2023).
\url{https://developer.apple.com/documentation/metalperformanceshaders}

\bibitem{Knuth1975}
Knuth, D.E., Moore, R.W.: An analysis of alpha-beta pruning.
Artificial Intelligence, 6(4), 293--326 (1975)

\bibitem{Silver2017}
Silver, D., et al.: Mastering chess and shogi by self-play with a general reinforcement learning algorithm.
arXiv preprint arXiv:1712.01815 (2017)

\bibitem{Campbell2002}
Campbell, M., Hoane Jr., A.J., Hsu, F.: Deep blue.
Artificial Intelligence, 134(1-2), 57--83 (2002)

\bibitem{NVIDIA2023}
NVIDIA Corporation: CUDA C++ programming guide.
Technical report, NVIDIA Corporation (2023)

\bibitem{Nickolls2008}
Nickolls, J., Buck, I., Garland, M., Skadron, K.: Scalable parallel programming with CUDA.
Queue, 6(2), 40--53 (2008)

\bibitem{Korf1985}
Korf, R.E.: Depth-first iterative-deepening: An optimal admissible tree search.
Artificial Intelligence, 27(1), 97--109 (1985)

\bibitem{Zobrist1970}
Zobrist, A.L.: A new hashing method with application for game playing.
ICCA Journal, 13(2), 69--73 (1970)

\bibitem{Beal1989}
Beal, D.F.: Experiments with the null move.
Advances in Computer Chess, 5, 65--79 (1989)

\bibitem{Heinz2000}
Heinz, E.A.: Adaptive null-move pruning.
ICGA Journal, 23(3), 123--132 (2000)

\bibitem{Coulom2006}
Coulom, R.: Efficient selectivity and backup operators in monte-carlo tree search.
In: Computers and Games, LNCS vol. 4630, pp. 72--83. Springer (2006)

\end{thebibliography}
\end{document}
