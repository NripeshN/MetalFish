%%%%%%%%%%%%%%%%%%%% MetalFish Paper %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% MetalFish: GPU-Accelerated Chess Engine on Apple Silicon
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{svproc}

\usepackage{url}
\def\UrlFont{\rmfamily}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% C++ code listing style
\lstdefinestyle{cppstyle}{
    language=C++,
    backgroundcolor=\color{gray!5},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=2,
    frame=single,
    keywordstyle=\color{blue!70!black},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red!60!black},
    morekeywords={uint64_t, int32_t, int16_t, int8_t, uint, device, kernel, constant}
}

\begin{document}
\mainmatter

\title{MetalFish: GPU-Accelerated NNUE Evaluation\\on Apple Silicon}

\titlerunning{MetalFish: GPU NNUE on Apple Silicon}

\author{Nripesh Niketan\inst{1}}

\authorrunning{N. Niketan}

\institute{Independent Researcher\\
\email{nripesh14@gmail.com}}

\maketitle

\begin{abstract}
We present MetalFish, a GPU-accelerated chess engine leveraging Apple Silicon's unified memory architecture for NNUE evaluation. Through systematic benchmarking on M2 Max, we demonstrate: (1) single-position (N=1) blocking latency of 281~$\mu$s median, (2) per-position cost of 0.4~$\mu$s at batch size 4096, (3) true single-dispatch batching with up to 250$\times$ speedup over sequential dispatches at N=256, and (4) SIMD-optimized kernels with 8-way unrolled accumulation and simdgroup operations. We support 64 features per perspective, and observed maximum 30 in our dataset. While synchronous blocking dispatch overhead (140~$\mu$s) makes single-position GPU evaluation unsuitable for alpha-beta search without speculative evaluation, batch evaluation becomes throughput-competitive at $N \geq 512$, enabling efficient bulk analysis and MCTS-style evaluation.

\keywords{Chess Engine, GPU Computing, Metal, NNUE, Apple Silicon, Unified Memory}
\end{abstract}

\section{Introduction}

Modern chess engines combine alpha-beta search with neural network evaluation (NNUE) to achieve superhuman playing strength. While GPU acceleration has proven effective for batch-oriented algorithms like Monte Carlo Tree Search in Leela Chess Zero~\cite{LeelaChessZero2024}, its applicability to traditional alpha-beta search remains challenging due to sequential evaluation patterns.

Apple Silicon's unified memory architecture eliminates explicit CPU-GPU memory transfers, potentially reducing the overhead that has historically limited GPU adoption in alpha-beta engines. This paper presents MetalFish, a GPU-accelerated chess engine that explores the practical limits of GPU evaluation on Apple Silicon.

\subsection{Contributions}

\begin{enumerate}
\item \textbf{Optimized GPU NNUE implementation}: We achieve 281~$\mu$s median single-position blocking latency and 0.4~$\mu$s per position at batch size 4096.

\item \textbf{Complete feature coverage}: We support 64 features per perspective (128 total), and observed maximum 30 per perspective in our benchmark dataset---well within the limit for all standard chess positions.

\item \textbf{Verified true batching}: We demonstrate single-dispatch batching achieving up to 250$\times$ speedup over sequential dispatches at batch size 256.

\item \textbf{SIMD-optimized kernels}: We present Metal compute kernels with 8-way unrolled accumulation, dual-perspective feature transformation, and threadgroup memory optimization.

\item \textbf{Comprehensive benchmarking}: We provide detailed stage breakdowns, latency percentiles, correctness verification, and scaling analysis across batch sizes 1--4096.
\end{enumerate}

\section{Background}

\subsection{NNUE Architecture}

Stockfish's NNUE~\cite{Stockfish2024,Nasu2018} uses HalfKAv2\_hm features with sparse input and efficient incremental updates. Table~\ref{tab:nnue_arch} summarizes the architecture.

\begin{table}[t]
\caption{NNUE Network Architecture}
\label{tab:nnue_arch}
\centering
\begin{tabular}{lrr}
\toprule
Component & Big Network & Small Network \\
\midrule
Feature set & HalfKAv2\_hm & HalfKAv2\_hm \\
Input features & 45,056 & 22,528 \\
Hidden dimension & 1,024 & 128 \\
FC0 output & 15 (+1 skip) & 15 (+1 skip) \\
FC1 output & 32 & 32 \\
FC2 output & 1 & 1 \\
Layer stacks (buckets) & 8 & 8 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Feature requirements}: HalfKAv2\_hm generates one feature per non-king piece from each perspective. A position with 30 pieces (excluding 2 kings) generates up to 30 features per perspective. We support 64 features per perspective, providing headroom for all legal positions.

\subsection{Metal Compute Model}

Apple Metal~\cite{AppleMetal2024} provides GPU compute through command buffers with unified memory access. Key characteristics:
\begin{itemize}
\item \textbf{Unified memory}: CPU and GPU share the same physical memory, eliminating explicit transfers
\item \textbf{Command buffer lifecycle}: Allocation, encoding, submission, and synchronization
\item \textbf{Threadgroup memory}: Fast on-chip memory for inter-thread communication
\item \textbf{SIMD groups}: 32-wide SIMD execution on Apple GPUs
\end{itemize}

\section{System Architecture}

\subsection{GPU Constants and Limits}

Table~\ref{tab:gpu_constants} shows the key GPU configuration parameters.

\begin{table}[t]
\caption{GPU Configuration Constants}
\label{tab:gpu_constants}
\centering
\begin{tabular}{lr}
\toprule
Parameter & Value \\
\midrule
Max batch size & 4,096 \\
Max features per perspective & 64 \\
Max total features & 128 \\
Threadgroup size & 256 \\
SIMD group size & 32 \\
Forward pass threads & 64 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Kernel Optimizations}

We implement three key optimizations in our Metal compute kernels:

\textbf{1. SIMD-aware feature transformation}: The feature transform kernel processes features with memory coalescing, accessing weights with stride patterns that maximize cache utilization.

\textbf{2. Unrolled accumulation}: The forward pass uses 8-way unrolled loops for the FC0 layer, reducing loop overhead and enabling instruction-level parallelism:

\begin{lstlisting}[style=cppstyle,caption={8-way unrolled FC0 accumulation}]
for (; i + 7 < hidden_dim; i += 8) {
    int8_t c0 = clipped_relu(acc[i] >> SCALE);
    int8_t c1 = clipped_relu(acc[i+1] >> SCALE);
    // ... c2-c7
    sum += c0 * weights[(i)*stride + out];
    sum += c1 * weights[(i+1)*stride + out];
    // ... 6 more
}
\end{lstlisting}

\textbf{3. Threadgroup memory for FC layers}: Intermediate results (FC0 outputs, skip connections) are stored in threadgroup memory, enabling efficient inter-thread communication without global memory round-trips.

\subsection{Batch Evaluation Pipeline}

Algorithm~\ref{alg:batch} shows the batch evaluation pipeline.

\begin{algorithm}[t]
\caption{GPU Batch NNUE Evaluation}
\label{alg:batch}
\begin{algorithmic}[1]
\Require Batch of $N$ positions
\Ensure Evaluation scores for all positions
\State \textbf{// Stage 1: Feature extraction (CPU)}
\For{each position}
    \State Extract white/black features from bitboards
    \State Store in contiguous buffers with offsets
\EndFor
\State \textbf{// Stage 2: GPU evaluation (single command buffer)}
\State Upload features to unified memory buffers
\State \Call{DispatchThreads}{$hidden\_dim \times N$} \Comment{Feature transform}
\State \Call{Barrier}{}
\State \Call{DispatchThreadgroups}{$N$, threads=64} \Comment{Forward pass}
\State \Call{SubmitAndWait}{}
\State \Return scores from output buffer
\end{algorithmic}
\end{algorithm}

\section{Experimental Methodology}

\subsection{Hardware and Software}

\begin{itemize}
\item \textbf{Hardware}: Apple M2 Max (12-core CPU, 38-core GPU, 64GB unified memory)
\item \textbf{Software}: macOS 14.0, Xcode 15.0, Metal 3.0
\item \textbf{Build}: CMake, -O3, LTO enabled
\item \textbf{Networks}: nn-c288c895ea92.nnue (125MB), nn-37f18f62d772.nnue (6MB)
\end{itemize}

\subsection{Benchmark Dataset}

Our benchmark uses 8 unique FEN positions representing diverse game phases (opening, middlegame, endgame), cycled to create 2048 test positions. Piece counts range from 2 to 32, with most positions containing 28--32 pieces.

\subsection{Timing Methodology}

All measurements use \texttt{std::chrono::high\_resolution\_clock}:
\begin{itemize}
\item \textbf{Warmup}: 100 iterations discarded
\item \textbf{Samples}: 100--100,000 iterations depending on variance
\item \textbf{Statistics}: Median, P95, P99 reported
\item \textbf{GPU timing}: Blocking \texttt{waitUntilCompleted()} (synchronous)
\end{itemize}

\section{Results}

\subsection{Feature Coverage}

Table~\ref{tab:feature_coverage} shows the feature count distribution.

\begin{table}[t]
\caption{Feature Count Distribution (2,048 positions)}
\label{tab:feature_coverage}
\centering
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Max features observed (total) & 60 \\
Max features per perspective & 30 \\
GPU limit per perspective & 64 \\
Positions exceeding limit & 0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{GPU Dispatch Overhead}

Table~\ref{tab:dispatch} shows minimal-kernel dispatch overhead.

\begin{table}[t]
\caption{GPU Dispatch Overhead---Minimal Kernel (N=1,000)}
\label{tab:dispatch}
\centering
\begin{tabular}{lr}
\toprule
Statistic & Latency ($\mu$s) \\
\midrule
Median & 140 \\
P95 & 245 \\
P99 & 332 \\
\bottomrule
\end{tabular}
\end{table}

The 140~$\mu$s median dispatch overhead represents the minimum cost for any GPU operation in synchronous blocking mode.

\subsection{GPU Stage Breakdown}

Table~\ref{tab:stage_breakdown} decomposes end-to-end latency.

\begin{table}[t]
\caption{GPU Stage Breakdown (N=100 iterations each)}
\label{tab:stage_breakdown}
\centering
\begin{tabular}{lrrr}
\toprule
Batch Size & CPU Prep & GPU Eval & GPU \% \\
\midrule
1 & 0.2 $\mu$s & 281 $\mu$s & 99.9\% \\
8 & 0.3 $\mu$s & 297 $\mu$s & 99.9\% \\
512 & 5.9 $\mu$s & 343 $\mu$s & 98.3\% \\
\bottomrule
\end{tabular}
\end{table}

GPU dispatch and kernel execution dominate ($>$98\% of total time). CPU feature extraction is negligible.

\subsection{Batch Latency Scaling}

Table~\ref{tab:batch_latency} shows end-to-end latency across batch sizes up to the maximum supported (4096).

\begin{table}[t]
\caption{GPU End-to-End Batch Latency (N=100 iterations)}
\label{tab:batch_latency}
\centering
\begin{tabular}{rrrrr}
\toprule
Batch & Median & P95 & P99 & Per-Pos \\
Size & ($\mu$s) & ($\mu$s) & ($\mu$s) & ($\mu$s) \\
\midrule
1 & 281 & 370 & 487 & 281.0 \\
8 & 297 & 397 & 425 & 37.2 \\
32 & 294 & 407 & 452 & 9.2 \\
128 & 261 & 376 & 481 & 2.0 \\
512 & 343 & 449 & 493 & 0.7 \\
1024 & 528 & 657 & 787 & 0.5 \\
2048 & 837 & 954 & 1,007 & 0.4 \\
3072 & 1,148 & 1,254 & 1,532 & 0.4 \\
4096 & 1,442 & 1,608 & 1,712 & 0.4 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings}: (1) Latency is approximately constant (261--297~$\mu$s) for batch sizes 1--128, showing dispatch dominance. (2) Per-position cost drops from 281~$\mu$s (N=1) to 0.4~$\mu$s (N=4096). (3) Latency increases linearly beyond N=512, indicating kernel compute becoming significant.

\subsection{True Batching Verification}

Table~\ref{tab:batching} compares sequential vs batched dispatches.

\begin{table}[t]
\caption{True Batching Verification (N=50 iterations)}
\label{tab:batching}
\centering
\begin{tabular}{rrrr}
\toprule
N & Sequential & Batched & Speedup \\
  & (N$\times$1 CB) & (1$\times$1 CB) & \\
\midrule
16 & 4,418 $\mu$s & 264 $\mu$s & 16.7$\times$ \\
64 & 17,643 $\mu$s & 275 $\mu$s & 64.2$\times$ \\
256 & 78,649 $\mu$s & 314 $\mu$s & 250.2$\times$ \\
1024 & 313,370 $\mu$s & 623 $\mu$s & 503.3$\times$ \\
\bottomrule
\end{tabular}
\end{table}

The sequential case creates N separate command buffers; the batched case uses one command buffer with two dispatches (feature transform + forward pass). Speedups scale linearly with batch size, confirming true single-dispatch batching.

\subsection{GPU Evaluation Correctness}

Table~\ref{tab:correctness} verifies GPU evaluation consistency.

\begin{table}[t]
\caption{GPU Evaluation Correctness (100 positions)}
\label{tab:correctness}
\centering
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Non-zero GPU scores & 100\% \\
Consistent across runs & 100\% \\
Mean $|$GPU score$|$ & 534 \\
\bottomrule
\end{tabular}
\end{table}

GPU evaluation produces consistent, non-zero scores across repeated runs. Note: GPU uses NNUE weights while CPU baseline uses simple material+PST, so absolute values differ.

\subsection{Search Performance}

The engine achieves 1.38M nodes/second using CPU NNUE evaluation:

\begin{table}[t]
\caption{Search Benchmark (50 positions, depth 13)}
\label{tab:search}
\centering
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Total Nodes & 2,477,446 \\
Total Time & 1,792 ms \\
Nodes/Second & 1,382,503 \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}

\subsection{When GPU Evaluation Helps}

GPU batch evaluation becomes throughput-competitive at $N \geq 512$ (0.7~$\mu$s/position). This enables:
\begin{itemize}
\item \textbf{Database analysis}: Evaluating thousands of positions from game databases
\item \textbf{MCTS evaluation}: Monte Carlo Tree Search naturally batches leaf evaluations
\item \textbf{Training data generation}: Bulk position evaluation for neural network training
\end{itemize}

\subsection{Alpha-Beta Limitations}

Single-position GPU blocking latency (281~$\mu$s) makes GPU evaluation unsuitable for alpha-beta search in synchronous blocking mode and without speculative evaluation. Alpha-beta search evaluates positions sequentially with data-dependent pruning, making batch accumulation impractical without significant architectural changes.

\subsection{Optimization Impact}

Our optimizations target three areas:
\begin{itemize}
\item \textbf{Memory access}: SIMD-aware coalesced access patterns
\item \textbf{Compute}: 8-way unrolled accumulation loops
\item \textbf{Communication}: Threadgroup memory for intermediate results
\end{itemize}

\section{Related Work}

Leela Chess Zero~\cite{LeelaChessZero2024} demonstrates successful GPU acceleration through MCTS, which naturally batches evaluations. AlphaZero~\cite{Silver2017} showed neural network evaluation can replace handcrafted evaluation with batch-oriented search.

For alpha-beta, Rocki and Suda~\cite{Rocki2010} explored GPU parallelization through parallel subtree evaluation. Our work extends this to unified memory hardware with optimized NNUE kernels.

Apple's Metal documentation~\cite{AppleMetal2024,AppleMetalBestPractices2024} recommends minimizing command buffer submissions and using threadgroup memory for intermediate results.

\section{Conclusion}

We presented MetalFish, a GPU-accelerated chess engine achieving:

\begin{enumerate}
\item \textbf{281~$\mu$s} median single-position blocking latency
\item \textbf{0.4~$\mu$s} per-position cost at batch size 4096
\item \textbf{250$\times$} true batching speedup at N=256
\item \textbf{100\%} GPU evaluation consistency
\item \textbf{1.38M} nodes/second search performance
\end{enumerate}

GPU acceleration is effective for batch-oriented workloads (MCTS, database analysis, training) but synchronous blocking dispatch overhead makes it unsuitable for alpha-beta's sequential evaluation pattern without speculative evaluation or asynchronous queuing. Our optimized Metal kernels provide a solid foundation for GPU-accelerated chess evaluation on Apple Silicon.

\subsection*{Reproducibility}

\textbf{Hardware}: Apple M2 Max, 64GB. \textbf{Software}: macOS 14.0, Xcode 15.0. \textbf{Build}: CMake, -O3, LTO. \textbf{Source}: \url{https://github.com/NripeshN/MetalFish}. \textbf{Benchmark}: \texttt{gpubench} UCI command.

\begin{thebibliography}{10}

\bibitem{Stockfish2024}
Stockfish Developers: Stockfish 16 NNUE documentation.
\url{https://github.com/official-stockfish/Stockfish} (2024)

\bibitem{LeelaChessZero2024}
Leela Chess Zero: Neural network based chess engine.
\url{https://lczero.org/} (2024)

\bibitem{Silver2017}
Silver, D., et al.: Mastering chess and shogi by self-play with a general reinforcement learning algorithm.
arXiv:1712.01815 (2017)

\bibitem{Rocki2010}
Rocki, K., Suda, R.: Parallel minimax tree searching on GPU.
In: Parallel Processing and Applied Mathematics, LNCS vol. 6067, pp. 449--456. Springer (2010)

\bibitem{Nasu2018}
Nasu, Y.: Efficiently updatable neural-network-based evaluation functions for computer shogi.
The 28th World Computer Shogi Championship Appeal Document (2018)

\bibitem{AppleMetal2024}
Apple Inc.: Metal Programming Guide.
\url{https://developer.apple.com/metal/} (2024)

\bibitem{AppleMetalBestPractices2024}
Apple Inc.: Metal Best Practices Guide.
\url{https://developer.apple.com/library/archive/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/} (2024)

\bibitem{Knuth1975}
Knuth, D.E., Moore, R.W.: An analysis of alpha-beta pruning.
Artificial Intelligence 6(4), 293--326 (1975)

\end{thebibliography}

\end{document}
