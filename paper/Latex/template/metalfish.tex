%%%%%%%%%%%%%%%%%%%% MetalFish Paper %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% MetalFish: GPU-Accelerated Chess Engine on Apple Silicon
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\def\UrlFont{\rmfamily}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{parskip}

% Adjust paragraph spacing
\setlength{\parskip}{0.8em plus 0.2em minus 0.1em}
\setlength{\parindent}{0pt}

% Improve list spacing
\usepackage{enumitem}
\setlist[itemize]{itemsep=0.3em, topsep=0.5em, parsep=0.2em}
\setlist[enumerate]{itemsep=0.3em, topsep=0.5em, parsep=0.2em}

% Add spacing around floats
\setlength{\floatsep}{1.5em plus 0.5em minus 0.3em}
\setlength{\textfloatsep}{1.5em plus 0.5em minus 0.3em}
\setlength{\intextsep}{1.2em plus 0.3em minus 0.2em}

% Spacing around equations
\AtBeginDocument{
  \setlength{\abovedisplayskip}{1em plus 0.3em minus 0.2em}
  \setlength{\belowdisplayskip}{1em plus 0.3em minus 0.2em}
  \setlength{\abovedisplayshortskip}{0.8em plus 0.2em}
  \setlength{\belowdisplayshortskip}{0.8em plus 0.2em}
}

% C++ code listing style
\lstdefinestyle{cppstyle}{
    language=C++,
    backgroundcolor=\color{gray!5},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=8pt,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=2,
    frame=single,
    framesep=4pt,
    xleftmargin=12pt,
    aboveskip=1em,
    belowskip=1em,
    rulecolor=\color{gray!40},
    keywordstyle=\color{blue!70!black},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red!60!black},
    morekeywords={uint64_t, int32_t, int16_t, int8_t, uint, device, kernel, constant, thread_position_in_grid}
}

\begin{document}
\mainmatter
%
\title{MetalFish: A GPU-Accelerated Chess Engine\\for Apple Silicon Unified Memory Architecture}
%
\titlerunning{MetalFish: GPU Chess Engine}
%
\author{Nripesh Niketan\inst{1}}
%
\authorrunning{N. Niketan}
%
\institute{Independent Researcher\\
\email{nripesh14@gmail.com}}

\maketitle

\begin{abstract}
We present MetalFish, a GPU-accelerated UCI chess engine that integrates Stockfish-style alpha-beta search with Apple Metal GPU acceleration on unified memory architecture. MetalFish implements a hybrid CPU-GPU design where the CPU executes traditional search algorithms including Principal Variation Search, null-move pruning, late move reductions, and singular extensions, while the GPU accelerates NNUE neural network evaluation through Metal compute shaders. The implementation leverages Apple Silicon's unified memory to achieve zero-copy data sharing between CPU search routines and GPU evaluation kernels. We describe the complete system architecture, present GPU kernel implementations for feature transformation and incremental accumulator updates, and report empirical results from our test suite. The engine correctly computes perft values for standard test positions, validates move generation through established benchmarks, and demonstrates functional GPU acceleration on Apple M-series processors. This work provides practical insights into the challenges and trade-offs of hybrid CPU-GPU chess engine design.

\keywords{Chess Engine, GPU Computing, Metal, NNUE, Unified Memory, Apple Silicon}
\end{abstract}

%
\section{Introduction}
%
Modern chess engines achieve remarkable playing strength through sophisticated alpha-beta search enhanced with neural network evaluation. Stockfish, the current state-of-the-art engine, combines Principal Variation Search (PVS) with Efficiently Updatable Neural Networks (NNUE) to achieve superhuman performance \cite{Romstad2004}. However, the emergence of GPU computing presents opportunities to accelerate evaluation-intensive components of chess engines.

The primary challenge in GPU-accelerated chess engines lies in the fundamental mismatch between alpha-beta search's sequential dependencies and GPU architectures' parallel execution model. When an alpha-beta cutoff occurs, remaining sibling nodes can be pruned, creating dependency chains that serialize computation. On GPUs, where threads execute in lockstep within warps, such branch divergence degrades performance significantly \cite{Nickolls2008}.

MetalFish addresses this challenge through a hybrid CPU-GPU architecture that leverages Apple Silicon's unified memory. Rather than attempting full GPU parallelization of alpha-beta search, we maintain traditional CPU-based search while offloading neural network evaluation to the GPU. The unified memory architecture eliminates the traditional GPU computing bottleneck of host-device data transfer, enabling efficient hybrid execution without explicit memory copies.

This paper presents the complete MetalFish implementation, describing the search algorithm integration, GPU kernel design, and unified memory utilization. We report empirical results demonstrating correct move generation through perft verification and functional GPU acceleration on Apple M-series hardware.

\section{Background and Related Work}

\subsection{Alpha-Beta Search and Principal Variation Search}

The minimax algorithm with alpha-beta pruning forms the foundation of modern chess engines \cite{Knuth1975}. Alpha-beta maintains bounds $[\alpha, \beta]$ representing the range of possible values for the current node; when $\alpha \geq \beta$, remaining siblings can be pruned without affecting the minimax value.

Principal Variation Search (PVS) refines alpha-beta by assuming the first move searched at each node is optimal \cite{Reinefeld1983}. Subsequent moves are searched with zero-width windows; if a move exceeds alpha, a full-window re-search is performed. This approach reduces the number of full evaluations while maintaining correctness.

\subsection{NNUE Evaluation}

Efficiently Updatable Neural Networks (NNUE) represent a paradigm shift in chess evaluation \cite{Nasu2018}. The architecture consists of a large sparse input layer representing piece-square combinations, followed by smaller dense layers. The key insight is that network activations can be updated incrementally as moves are made and unmade, rather than recomputing from scratch.

\subsection{GPU-Based Chess Engines}

Leela Chess Zero (Lc0) pioneered GPU-accelerated neural network evaluation combined with Monte Carlo Tree Search (MCTS) \cite{LeelaChessZero2024}. Unlike alpha-beta, MCTS is naturally suited to GPU parallelization as individual simulations can execute independently. However, alpha-beta's tactical precision through deep selective search motivates hybrid approaches combining traditional search with GPU acceleration.

\subsection{Unified Memory Architecture}

Apple Silicon's unified memory architecture allows CPU and GPU to access the same physical memory coherently \cite{Apple2023}. This eliminates explicit data transfers between separate memory spaces, enabling algorithms that leverage both sequential CPU processing and parallel GPU capabilities without transfer overhead.

\section{System Architecture}

\subsection{Design Overview}

MetalFish implements a hybrid CPU-GPU architecture with three primary components:

\textbf{CPU Search Engine:} Executes the complete alpha-beta search algorithm including PVS, move ordering, and all pruning techniques. Search control flow remains entirely on CPU to avoid branch divergence penalties.

\textbf{GPU Evaluation Engine:} Implements NNUE inference through Metal compute shaders, processing feature transformation and network forward passes on GPU.

\textbf{Unified Memory Interface:} Manages shared buffers accessible by both CPU and GPU without explicit copying, using Metal's shared storage mode.

\subsection{Search Algorithm Implementation}

The search implementation follows Stockfish's architecture, incorporating the following techniques:

\subsubsection{Move Ordering}

Effective move ordering is critical for alpha-beta efficiency. MetalFish implements multiple history heuristics:

\textbf{Butterfly History:} Tracks quiet move success by source and destination squares, stored in a $[2][4096]$ array indexed by $[\text{color}][\text{from} \times 64 + \text{to}]$. Values are initialized to 68 following Stockfish conventions.

\textbf{Capture History:} Tracks capture move success by piece, destination square, and captured piece type, stored in a $[16][64][8]$ array. Values are initialized to $-689$.

\textbf{Continuation History:} Tracks move sequence success using the previous moves in the search stack. The implementation uses a $[16][64][16][64]$ array with values initialized to $-529$.

\textbf{Killer Moves:} Stores refutation moves per ply that caused beta cutoffs in sibling nodes.

\textbf{Counter Moves:} Stores moves that refute specific previous moves.

\subsubsection{Search Extensions}

\textbf{Singular Extension:} When a transposition table move appears significantly better than alternatives, search depth is extended. The implementation uses double and triple extensions for strongly singular moves.

\textbf{Check Extension:} Positions where the side to move is in check receive depth extensions to ensure tactical threats are fully resolved.

\subsubsection{Pruning Techniques}

\textbf{Null Move Pruning:} If passing a turn (null move) still produces a beta cutoff at reduced depth, the position is pruned. Verification search is applied at high depths.

\textbf{Late Move Reductions (LMR):} Moves searched later in the move list are searched at reduced depth. The reduction formula follows Stockfish:
\begin{equation}
R = \left\lfloor \frac{2747}{128} \times \ln(\text{moveNumber}) \right\rfloor
\end{equation}
with adjustments based on history scores, node type, and whether the position is in check.

\textbf{Futility Pruning:} Near-leaf nodes with static evaluations far below alpha are pruned when no single move is likely to raise the score sufficiently.

\textbf{SEE Pruning:} Moves with negative Static Exchange Evaluation are pruned or reduced based on depth and move type.

\textbf{ProbCut:} Positions are pruned when a shallow search of captures suggests the score will exceed beta by a significant margin.

\subsubsection{Transposition Table}

The transposition table stores previously searched positions using Zobrist hashing \cite{Zobrist1970}. Each entry contains the position hash, evaluation bound type (exact, lower, or upper), best move, search depth, and generation counter for aging. The replacement scheme considers entry depth and age to balance between preserving deep searches and allowing new entries.

\subsection{GPU NNUE Implementation}

The GPU evaluation engine implements NNUE inference through Metal compute shaders. The network architecture processes 1024-dimensional accumulators through fully-connected layers.

\subsubsection{Feature Extraction}

Position features are extracted using the HalfKAv2 scheme, where features are indexed by king position and piece-square combinations. For each piece on the board, the feature index is computed as:
\begin{equation}
\text{feature} = \text{king\_sq} \times 641 + \text{piece\_sq} \times 10 + \text{piece\_type}
\end{equation}
where piece positions are mirrored based on the perspective (white or black).

\subsubsection{Feature Transformation Kernel}

The feature transformer converts sparse input features to dense accumulator values. Each GPU thread computes one element of the output accumulator, iterating over all active features and accumulating the corresponding weight values.

\begin{lstlisting}[style=cppstyle, caption={Feature transformation kernel computing accumulator values from sparse input features}]
kernel void feature_transform(
    device const int16_t* weights,
    device const int16_t* biases,
    device const int* features,
    device int32_t* accumulator,
    constant int& num_features,
    constant int& ft_dims,
    uint h [[thread_position_in_grid]])
{
    if (h >= ft_dims) return;
    
    int32_t sum = biases[h];
    for (int i = 0; i < num_features; i++) {
        int f = features[i];
        sum += weights[f * ft_dims + h];
    }
    accumulator[h] = sum;
}
\end{lstlisting}

\subsubsection{Incremental Update Kernel}

When a move is made, only the changed features need updating rather than recomputing the entire accumulator. The kernel processes added and removed features in parallel.

\begin{lstlisting}[style=cppstyle, caption={Incremental accumulator update kernel processing added and removed features}]
kernel void feature_update(
    device const int16_t* weights,
    device int32_t* accumulator,
    device const int* added_features,
    device const int* removed_features,
    constant int& num_added,
    constant int& num_removed,
    constant int& ft_dims,
    uint h [[thread_position_in_grid]])
{
    if (h >= ft_dims) return;
    
    int32_t delta = 0;
    for (int i = 0; i < num_added; i++) {
        int f = added_features[i];
        delta += weights[f * ft_dims + h];
    }
    for (int i = 0; i < num_removed; i++) {
        int f = removed_features[i];
        delta -= weights[f * ft_dims + h];
    }
    accumulator[h] += delta;
}
\end{lstlisting}

\subsubsection{Network Forward Pass}

The NNUE forward pass processes the accumulated features through fully-connected layers with clipped ReLU activations:
\begin{equation}
\text{clipped\_relu}(x) = \min(\max(x \gg 6, 0), 127)
\end{equation}

The network architecture consists of:
\begin{itemize}
\item FC0: $2048 \rightarrow 16$ (concatenated white/black accumulators)
\item FC1: $15 \rightarrow 32$ with squared clipped ReLU
\item FC2: $32 \rightarrow 1$ producing the final evaluation score
\end{itemize}

\subsection{Metal Backend Implementation}

The Metal backend manages GPU resources and kernel execution. The device initialization establishes the GPU context and command queue.

\begin{lstlisting}[style=cppstyle, caption={Metal device initialization establishing GPU context and command queue}]
Device::Device() {
    device_ = MTL::CreateSystemDefaultDevice();
    if (!device_) {
        throw std::runtime_error(
            "Failed to create Metal device");
    }
    queue_ = device_->newCommandQueue();
    architecture_ = device_->name()->utf8String();
}
\end{lstlisting}

Buffer allocation uses shared storage mode to enable unified memory access, allowing both CPU code (via \texttt{buffer->contents()}) and GPU kernels to access the same memory without explicit transfers.

\begin{lstlisting}[style=cppstyle, caption={Buffer allocation with shared storage mode for unified memory access}]
bool GPUNNUEWeights::allocate(MTL::Device* device) {
    MTL::ResourceOptions options = 
        MTL::ResourceStorageModeShared;
    
    ft_weights = device->newBuffer(
        FT_IN_DIMS * FT_OUT_DIMS * sizeof(int16_t),
        options);
    ft_biases = device->newBuffer(
        FT_OUT_DIMS * sizeof(int16_t),
        options);
    return ft_weights && ft_biases;
}
\end{lstlisting}

\section{Experimental Results}

\subsection{Move Generation Verification}

Move generation correctness is verified through perft (performance test) calculations, which count the number of leaf nodes at a given depth. Table~\ref{tab:perft} shows results for the standard starting position.

\begin{table}[H]
\caption{Perft results for the standard chess starting position. All values match established correct results, validating move generation and position update logic.}
\label{tab:perft}
\centering
\begin{tabular}{rr}
\toprule
Depth & Nodes \\
\midrule
1 & 20 \\
2 & 400 \\
3 & 8,902 \\
4 & 197,281 \\
5 & 4,865,609 \\
6 & 119,060,324 \\
\bottomrule
\end{tabular}
\end{table}

Additional perft tests verify correctness for complex positions including the Kiwipete position (depth 5: 193,690,690 nodes), positions with en passant, castling rights, and promotion scenarios.

\subsection{GPU Backend Validation}

Table~\ref{tab:gpu_info} presents hardware characteristics reported by the Metal backend during initialization on an Apple M2 Max system.

\begin{table}[H]
\caption{GPU backend characteristics reported during MetalFish initialization on Apple M2 Max hardware.}
\label{tab:gpu_info}
\centering
\begin{tabular}{lr}
\toprule
Property & Value \\
\midrule
Device Name & Apple M2 Max \\
Unified Memory & Enabled \\
Maximum Buffer Size & 19,169 MB \\
Maximum Threadgroup Memory & 32,768 bytes \\
Maximum Threads per Threadgroup & 1,024 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Test Suite Results}

MetalFish includes unit tests covering core functionality. Table~\ref{tab:tests} summarizes the test categories and results.

\begin{table}[H]
\caption{Test suite results validating MetalFish components. All tests pass, confirming correct implementation of core functionality.}
\label{tab:tests}
\centering
\begin{tabular}{lr}
\toprule
Test Category & Result \\
\midrule
Bitboard Operations & Pass \\
Position Representation & Pass \\
Move Generation & Pass \\
Search Components (History, SEE, TT) & Pass \\
Metal GPU Backend & Pass \\
GPU NNUE Kernels & Pass \\
UCI Protocol Handling & Pass \\
Perft Verification & Pass \\
\bottomrule
\end{tabular}
\end{table}

\subsection{GPU Memory Utilization}

Table~\ref{tab:memory} shows GPU memory allocation for NNUE evaluation, as reported by the engine during initialization.

\begin{table}[H]
\caption{GPU memory allocation for NNUE evaluation components.}
\label{tab:memory}
\centering
\begin{tabular}{lr}
\toprule
Component & Allocation \\
\midrule
Feature Transformer Weights & 45,056 KB \\
Feature Transformer Biases & 2 KB \\
PSQT Weights & 704 KB \\
FC Layer Weights & 137 KB \\
Working Buffers & 2,296 KB \\
\midrule
Total GPU Memory & 48,195 KB \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}

\subsection{Hybrid Execution Trade-offs}

Our implementation reveals important trade-offs in hybrid CPU-GPU chess engine design:

\textbf{Kernel Dispatch Overhead:} GPU kernel dispatch and synchronization introduce latency that can exceed computation time for single-position evaluation. The Metal command buffer creation, encoding, commit, and wait cycle imposes overhead that favors CPU execution for individual positions.

\textbf{Batch Amortization:} GPU acceleration provides benefit when evaluating multiple positions simultaneously, amortizing dispatch overhead across the batch. Our observations suggest batch sizes of 8--16 positions or more are needed for GPU execution to outperform CPU.

\textbf{Unified Memory Advantage:} The unified memory architecture eliminates explicit data copying between CPU and GPU address spaces. This enables the hybrid approach where CPU search code and GPU evaluation share data structures directly through pointer access.

\subsection{Search Algorithm Considerations}

The sequential nature of alpha-beta search with pruning fundamentally limits GPU parallelization. Cutoff decisions depend on results from previously searched moves, creating data dependencies that serialize execution. Our hybrid approach accepts this limitation by keeping search on CPU while accelerating the evaluation function.

The history heuristics (butterfly, capture, continuation) require frequent small updates during search that would incur significant GPU synchronization overhead. Maintaining these tables on CPU and accessing them directly avoids this overhead.

\subsection{Limitations}

Several limitations affect the current implementation:

\begin{itemize}
\item GPU acceleration is limited to evaluation; the search algorithm executes entirely on CPU
\item Single-position evaluation is faster on CPU due to kernel dispatch overhead
\item The implementation uses pre-trained NNUE networks; training is not included
\item Syzygy tablebase probing interface is implemented but file loading is not complete
\end{itemize}

\section{Related Work}

Fat Fritz combined Stockfish's alpha-beta search with GPU-accelerated neural network evaluation, demonstrating the viability of hybrid approaches. Our work extends this direction to unified memory architectures where CPU-GPU data sharing is more efficient.

The MLX framework from Apple provides high-level abstractions for GPU computing on Apple Silicon. MetalFish uses lower-level Metal APIs for finer control over kernel execution and memory management.

\section{Conclusion}

MetalFish demonstrates that hybrid CPU-GPU chess engines are practical on unified memory architectures. By maintaining alpha-beta search on CPU while accelerating NNUE evaluation on GPU, we preserve the tactical precision of traditional search while leveraging GPU parallelism for neural network inference.

The unified memory architecture of Apple Silicon proves valuable for this hybrid approach, enabling zero-copy data sharing between CPU search routines and GPU evaluation kernels. This eliminates the transfer overhead that would otherwise penalize fine-grained CPU-GPU cooperation.

Key findings from this implementation:

\begin{enumerate}
\item Alpha-beta search's sequential dependencies make full GPU parallelization impractical; hybrid approaches are more effective
\item Unified memory enables efficient CPU-GPU cooperation without explicit data transfers
\item GPU kernel dispatch overhead favors CPU execution for single-position evaluation
\item Batch processing amortizes GPU overhead, providing benefit for multi-position evaluation
\end{enumerate}

The MetalFish implementation provides a foundation for future research in GPU-accelerated game tree search on unified memory systems.

\section*{Acknowledgments}

The author thanks the Stockfish development team for their open-source contributions that informed this implementation, and the Leela Chess Zero team for advancing neural network chess evaluation.

\newpage

%
% ---- Bibliography ----
%
\begin{thebibliography}{15}
%

\bibitem{Romstad2004}
Romstad, T., Costalba, M., Kiiski, J.: Stockfish: A strong open source chess engine.
\url{https://stockfishchess.org/} (2008)

\bibitem{Nickolls2008}
Nickolls, J., Buck, I., Garland, M., Skadron, K.: Scalable parallel programming with CUDA.
Queue, 6(2), 40--53 (2008)

\bibitem{Knuth1975}
Knuth, D.E., Moore, R.W.: An analysis of alpha-beta pruning.
Artificial Intelligence, 6(4), 293--326 (1975)

\bibitem{Reinefeld1983}
Reinefeld, A.: An improvement to the scout tree-search algorithm.
ICCA Journal, 6(4), 4--14 (1983)

\bibitem{Nasu2018}
Nasu, Y.: NNUE: Efficiently updatable neural networks for board game position evaluation.
Master's Thesis, University of Electro-Communications (2018)

\bibitem{LeelaChessZero2024}
Leela Chess Zero Team: Leela chess zero: Neural network chess engine.
\url{https://lczero.org/} (2024)

\bibitem{Apple2023}
Apple Inc.: Metal programming guide.
Technical report, Apple Inc. (2023).
\url{https://developer.apple.com/metal/}

\bibitem{Zobrist1970}
Zobrist, A.L.: A new hashing method with application for game playing.
ICCA Journal, 13(2), 69--73 (1970)

\bibitem{Campbell2002}
Campbell, M., Hoane Jr., A.J., Hsu, F.: Deep blue.
Artificial Intelligence, 134(1-2), 57--83 (2002)

\bibitem{Silver2017}
Silver, D., et al.: Mastering chess and shogi by self-play with a general reinforcement learning algorithm.
arXiv preprint arXiv:1712.01815 (2017)

\bibitem{Korf1985}
Korf, R.E.: Depth-first iterative-deepening: An optimal admissible tree search.
Artificial Intelligence, 27(1), 97--109 (1985)

\bibitem{Beal1989}
Beal, D.F.: Experiments with the null move.
Advances in Computer Chess, 5, 65--79 (1989)

\bibitem{Heinz2000}
Heinz, E.A.: Adaptive null-move pruning.
ICGA Journal, 23(3), 123--132 (2000)

\end{thebibliography}
\end{document}
