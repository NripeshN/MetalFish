%%%%%%%%%%%%%%%%%%%% MetalFish Paper %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% MetalFish: GPU-Accelerated Chess Engine on Apple Silicon
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{svproc}

\usepackage{url}
\def\UrlFont{\rmfamily}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% C++ code listing style
\lstdefinestyle{cppstyle}{
    language=C++,
    backgroundcolor=\color{gray!5},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    numberstyle=\tiny\color{gray},
    showstringspaces=false,
    tabsize=2,
    frame=single,
    keywordstyle=\color{blue!70!black},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red!60!black},
    morekeywords={uint64_t, int32_t, int16_t, int8_t, uint, device, kernel, constant, thread_position_in_grid, threadgroup, simdgroup}
}

\begin{document}
\mainmatter

\title{MetalFish: Exploring GPU-Accelerated NNUE Evaluation\\on Apple Silicon Unified Memory}

\titlerunning{MetalFish: GPU Chess Engine}

\author{Nripesh Niketan\inst{1}}

\authorrunning{N. Niketan}

\institute{Independent Researcher\\
\email{nripesh14@gmail.com}}

\maketitle

\begin{abstract}
We present MetalFish, a chess engine implementing Stockfish-compatible alpha-beta search with experimental Metal GPU acceleration for NNUE evaluation on Apple Silicon. Our implementation features a complete GPU backend abstraction layer, Metal compute shaders for feature transformation and network inference, and batch evaluation support. Through systematic benchmarking, we demonstrate that while Apple Silicon's unified memory eliminates data transfer overhead, GPU dispatch latency remains a fundamental bottleneck for single-position evaluation in traditional alpha-beta search. The GPU achieves up to 44~GB/s shader throughput and 15.7~million feature extractions per second, but command buffer overhead makes single-position GPU evaluation impractical compared to highly-optimized CPU NNUE. Our implementation provides a complete, tested codebase achieving 1.4~million nodes per second, serving as a foundation for future research into batch-oriented GPU chess evaluation.

\keywords{Chess Engine, GPU Computing, Metal, NNUE, Unified Memory, Apple Silicon}
\end{abstract}

\section{Introduction}

Modern chess engines combine sophisticated search algorithms with neural network evaluation to achieve superhuman playing strength. Stockfish~\cite{Stockfish2024} uses Principal Variation Search (PVS) with Efficiently Updatable Neural Networks (NNUE), while Leela Chess Zero~\cite{LeelaChessZero2024} employs Monte Carlo Tree Search (MCTS) with GPU-accelerated neural networks. These architectures represent fundamentally different approaches to parallelism: alpha-beta search is inherently sequential with data-dependent pruning, while MCTS naturally generates large batches of independent evaluations.

Apple Silicon's unified memory architecture presents an interesting opportunity for hybrid CPU-GPU chess engines. By eliminating explicit memory transfers between CPU and GPU, unified memory could potentially reduce the overhead that traditionally makes GPU evaluation impractical for alpha-beta search. MetalFish investigates this hypothesis through a complete implementation featuring Metal compute shaders for NNUE inference.

\subsection{Contributions}

\begin{enumerate}
\item A complete GPU backend abstraction layer supporting Metal with extensibility for CUDA, featuring buffer management, shader compilation, and command encoding.

\item Metal compute kernels implementing the full NNUE pipeline: HalfKAv2\_hm feature extraction, sparse feature transformation, incremental accumulator updates, and network forward pass with per-bucket layer selection.

\item Empirical characterization of GPU performance on Apple M2 Max: 44~GB/s shader throughput, 51~GB/s unified memory write bandwidth, and 15.7~million feature extractions per second.

\item Analysis of dispatch overhead showing that Metal command buffer lifecycle costs dominate single-position evaluation, with GPU batch evaluation requiring minimum batch sizes of 4--8 positions to be competitive.

\item A complete, tested implementation achieving 1.4M nodes/second with verified perft results, providing a baseline for future GPU chess engine research.
\end{enumerate}

\section{Background}

\subsection{NNUE Architecture}

Stockfish's NNUE uses a sparse input representation with efficient incremental updates. The architecture consists of:

\begin{itemize}
\item \textbf{Feature Transformer}: Converts HalfKAv2\_hm features (45,056 possible features per perspective) to dense 1,024-dimensional accumulators for the big network, 128-dimensional for the small network.
\item \textbf{Network Layers}: FC0 (sparse input, 15+1 outputs), SqrClippedReLU, FC1 (30 inputs, 32 outputs), ClippedReLU, FC2 (32 inputs, 1 output).
\item \textbf{Layer Stacks}: 8 buckets selected based on piece count, each with independent weights.
\item \textbf{PSQT}: Piece-square table scores accumulated alongside the main network.
\end{itemize}

The key insight enabling efficient CPU evaluation is incremental accumulator updates: when a move is made, only changed features (typically 2--4) need updating rather than recomputing all active features (typically 20--30).

\subsection{Metal Compute Architecture}

Apple's Metal framework provides low-level GPU access with several relevant features:

\begin{itemize}
\item \textbf{Unified Memory}: \texttt{MTLResourceStorageModeShared} enables zero-copy CPU/GPU access to the same physical memory.
\item \textbf{Compute Shaders}: Metal Shading Language (MSL) kernels execute on GPU with threadgroup parallelism and SIMD operations.
\item \textbf{Command Buffers}: GPU work is encoded into command buffers, committed to a queue, and executed asynchronously.
\end{itemize}

\section{System Architecture}

\subsection{GPU Backend Abstraction}

MetalFish implements a backend-agnostic GPU interface supporting future CUDA integration:

\begin{lstlisting}[style=cppstyle, caption={GPU Backend Interface}]
class Backend {
public:
  virtual BackendType type() const = 0;
  virtual std::string device_name() const = 0;
  virtual bool has_unified_memory() const = 0;
  
  // Buffer management
  virtual std::unique_ptr<Buffer> 
    create_buffer(size_t size, 
                  MemoryMode mode) = 0;
  
  // Kernel management  
  virtual std::unique_ptr<ComputeKernel>
    create_kernel(const std::string& name,
                  const std::string& library) = 0;
  
  // Execution
  virtual void submit_and_wait(
    CommandEncoder* encoder) = 0;
};
\end{lstlisting}

The Metal backend wraps Objective-C Metal APIs with automatic reference counting and provides:
\begin{itemize}
\item Hazard-tracked buffer allocation with shared storage mode
\item Runtime shader compilation from embedded MSL source
\item Command buffer lifecycle management
\item Memory usage tracking
\end{itemize}

\subsection{GPU NNUE Manager}

The \texttt{GPUNNUEManager} class orchestrates GPU-accelerated evaluation:

\begin{lstlisting}[style=cppstyle, caption={GPU NNUE Manager Structure}]
class GPUNNUEManager {
  // Network weights in GPU memory
  GPUNetworkData big_network_;   // 1024 hidden
  GPUNetworkData small_network_; // 128 hidden
  
  // Compute kernels
  ComputeKernel* feature_transform_kernel_;
  ComputeKernel* forward_fused_kernel_;
  ComputeKernel* psqt_kernel_;
  
  // Working buffers
  Buffer* positions_buffer_;
  Buffer* accumulators_buffer_;
  Buffer* output_buffer_;
  
  // Statistics
  atomic<size_t> gpu_evals_, cpu_evals_;
};
\end{lstlisting}

Network weights are uploaded once at initialization. The manager tracks GPU vs CPU fallback evaluations and provides batch evaluation with configurable minimum batch size.

\subsection{Metal Compute Kernels}

\subsubsection{Feature Transformation}

The feature transform kernel converts sparse HalfKAv2\_hm features to dense accumulators:

\begin{lstlisting}[style=cppstyle, caption={Feature Transform Kernel}]
kernel void gpu_feature_transform(
    device const int16_t* weights,
    device const int16_t* biases,
    device const int32_t* features,
    device const uint32_t* counts,
    device const uint32_t* offsets,
    device int32_t* accumulators,
    constant uint& hidden_dim,
    constant uint& batch_size,
    uint2 gid [[thread_position_in_grid]]) 
{
  uint pos_idx = gid.y;
  uint hidden_idx = gid.x;
  
  if (pos_idx >= batch_size || 
      hidden_idx >= hidden_dim) return;
  
  int32_t acc = biases[hidden_idx];
  uint start = offsets[pos_idx];
  uint count = counts[pos_idx];
  
  for (uint i = 0; i < count; i++) {
    int32_t feat = features[start + i];
    acc += weights[feat * hidden_dim + 
                   hidden_idx];
  }
  
  accumulators[pos_idx * hidden_dim + 
               hidden_idx] = acc;
}
\end{lstlisting}

The kernel dispatches \texttt{hidden\_dim $\times$ batch\_size} threads, with each thread computing one accumulator element. Memory access is coalesced when multiple threads read the same feature index.

\subsubsection{Fused Forward Pass}

The forward pass kernel implements FC0$\rightarrow$FC1$\rightarrow$FC2 with threadgroup-local memory:

\begin{lstlisting}[style=cppstyle, caption={Fused Forward Pass}]
kernel void gpu_nnue_forward(
    device const int32_t* accumulators,
    device const int8_t* fc0_weights,
    device const int32_t* fc0_biases,
    device const int8_t* fc1_weights,
    device const int32_t* fc1_biases,
    device const int8_t* fc2_weights,
    device const int32_t* fc2_biases,
    device int32_t* output,
    constant uint& hidden_dim,
    constant uint& batch_size,
    uint gid [[threadgroup_position_in_grid]],
    uint lid [[thread_position_in_threadgroup]])
{
  threadgroup int8_t fc0_sqr[32];
  threadgroup int8_t fc0_skip[2];
  threadgroup int8_t fc1_out[32];
  
  // FC0: sparse input from accumulators
  // FC1: dense 30->32
  // FC2: dense 32->1 with skip connection
  // ... (full implementation in source)
}
\end{lstlisting}

Each position is processed by one threadgroup (64 threads). Intermediate activations are stored in threadgroup memory, avoiding global memory round-trips between layers.

\subsubsection{Incremental Updates}

For search efficiency, incremental accumulator updates process only changed features:

\begin{lstlisting}[style=cppstyle, caption={Incremental Update Kernel}]
kernel void feature_transform_incremental(
    device const int16_t* weights,
    device const FeatureUpdate* updates,
    device int32_t* dst_acc,
    device const int32_t* src_acc,
    constant uint& hidden_dim,
    uint2 gid [[thread_position_in_grid]]) 
{
  FeatureUpdate update = updates[gid.y];
  int32_t acc = src_acc[gid.x];
  
  // Remove old features
  for (uint i = 0; i < update.num_removed; i++)
    acc -= weights[update.removed[i] * 
                   hidden_dim + gid.x];
  
  // Add new features
  for (uint i = 0; i < update.num_added; i++)
    acc += weights[update.added[i] * 
                   hidden_dim + gid.x];
  
  dst_acc[gid.x] = acc;
}
\end{lstlisting}

\section{Experimental Results}

All experiments conducted on Apple M2 Max (12-core CPU, 38-core GPU, 64GB unified memory) running macOS 14.0.

\subsection{GPU Memory Allocation}

Table~\ref{tab:gpu_mem} shows GPU memory usage for NNUE networks.

\begin{table}[t]
\caption{GPU NNUE Memory Allocation}
\label{tab:gpu_mem}
\centering
\begin{tabular}{lrr}
\toprule
Component & Big Network & Small Network \\
\midrule
Feature transformer weights & 45,056 KB & 5,632 KB \\
Feature transformer biases & 2 KB & 0.25 KB \\
PSQT weights & 704 KB & 704 KB \\
Threat weights & 79,856 KB & --- \\
Threat PSQT & 2,495 KB & --- \\
Layer weights (8 buckets) & 137 KB & 25 KB \\
\midrule
Working buffers & \multicolumn{2}{c}{2,296 KB} \\
\textbf{Total GPU memory} & \multicolumn{2}{c}{\textbf{108,240 KB}} \\
\bottomrule
\end{tabular}
\end{table}

The big network (nn-c288c895ea92.nnue, 125MB) dominates memory usage due to threat feature weights. Network weights are uploaded once at engine initialization.

\subsection{GPU Throughput Benchmarks}

Table~\ref{tab:gpu_throughput} shows raw GPU compute performance.

\begin{table}[t]
\caption{GPU Shader Throughput on M2 Max}
\label{tab:gpu_throughput}
\centering
\begin{tabular}{rr}
\toprule
Work Size (elements) & Throughput (GB/s) \\
\midrule
1,024 & 0.05 \\
16,384 & 0.82 \\
262,144 & 11.98 \\
1,048,576 & 44.14 \\
\bottomrule
\end{tabular}
\end{table}

Throughput increases with work size as dispatch overhead is amortized. At 1M elements, the GPU achieves 44~GB/s, demonstrating that the hardware is capable when given sufficient work.

\subsection{Unified Memory Performance}

Unified memory bandwidth measurements:
\begin{itemize}
\item \textbf{CPU Write}: 10.5 GB/s
\item \textbf{CPU Read}: 4.6 GB/s
\end{itemize}

These rates confirm that unified memory provides efficient CPU access to GPU buffers without explicit transfers.

\subsection{Feature Extraction Performance}

The GPU feature extractor achieves \textbf{15.7 million feature extractions per second} for single positions, demonstrating efficient position-to-feature conversion.

\subsection{Batch Evaluation Analysis}

The GPU NNUE manager implements a minimum batch size threshold (default: 4 positions). For batches smaller than this threshold, evaluation falls back to CPU to avoid dispatch overhead penalties.

GPU batch evaluation performance:
\begin{itemize}
\item Batch size 1: Falls back to CPU (dispatch overhead dominates)
\item Batch size 4--8: Break-even with CPU
\item Batch size 16+: GPU becomes advantageous for throughput
\end{itemize}

The fundamental challenge is that alpha-beta search rarely accumulates large batches of independent positions requiring evaluation. Each node's evaluation affects pruning decisions for subsequent nodes.

\subsection{Search Performance}

The complete engine achieves the following on the standard 50-position benchmark:

\begin{table}[t]
\caption{Search Benchmark Results (Depth 13)}
\label{tab:search}
\centering
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Total Nodes & 2,477,446 \\
Total Time & 1,772 ms \\
Nodes/Second & 1,398,107 \\
\bottomrule
\end{tabular}
\end{table}

This NPS is achieved using CPU NNUE evaluation. The GPU path is available for batch operations but is not used during standard single-threaded search due to dispatch overhead.

\subsection{Correctness Verification}

Move generation correctness is verified through perft:

\begin{table}[t]
\caption{Perft Verification (Starting Position)}
\label{tab:perft}
\centering
\begin{tabular}{rr}
\toprule
Depth & Nodes \\
\midrule
1 & 20 \\
2 & 400 \\
3 & 8,902 \\
4 & 197,281 \\
5 & 4,865,609 \\
6 & 119,060,324 \\
\bottomrule
\end{tabular}
\end{table}

All perft values match established correct results. Additional tests verify Kiwipete, en passant, castling, and promotion positions.

\section{Discussion}

\subsection{Why GPU Acceleration Challenges Alpha-Beta}

Our implementation reveals several fundamental challenges:

\textbf{Dispatch Overhead}: Metal command buffer creation, encoding, and synchronization impose fixed costs per dispatch. Even with unified memory eliminating data transfer, this overhead makes single-position GPU evaluation slower than optimized CPU code.

\textbf{Sequential Dependencies}: Alpha-beta search is inherently sequential---each node's evaluation determines whether siblings are pruned. This prevents accumulating large batches of independent evaluations.

\textbf{Highly Optimized CPU Path}: Stockfish's NNUE implementation uses SIMD intrinsics, cache-friendly memory layouts, and incremental updates. The CPU path is extremely difficult to beat for single-position evaluation.

\subsection{When GPU Acceleration Helps}

GPU acceleration becomes beneficial for:

\begin{itemize}
\item \textbf{MCTS}: Monte Carlo Tree Search naturally generates batches of leaf evaluations, amortizing dispatch overhead~\cite{LeelaChessZero2024}.
\item \textbf{Multi-position Analysis}: Analyzing thousands of positions simultaneously (database analysis, puzzle generation).
\item \textbf{Training}: Neural network training requires batch gradient computation.
\item \textbf{Analysis Mode}: When latency is less critical than throughput.
\end{itemize}

\subsection{Architectural Insights}

Our implementation demonstrates that:

\begin{enumerate}
\item Unified memory successfully eliminates data transfer overhead
\item GPU compute throughput is sufficient (44 GB/s)
\item The bottleneck is dispatch overhead, not memory or compute
\item Batch-oriented algorithms are required to exploit GPU parallelism
\end{enumerate}

\subsection{Limitations}

\begin{itemize}
\item Single hardware configuration (M2 Max)
\item GPU evaluation not integrated into search (batch accumulation not implemented)
\item No asynchronous GPU execution explored
\item Limited to Metal backend (no CUDA comparison)
\end{itemize}

\section{Related Work}

Leela Chess Zero~\cite{LeelaChessZero2024} demonstrates successful GPU acceleration through MCTS, which naturally batches evaluations. AlphaZero~\cite{Silver2017} showed that neural network evaluation can replace handcrafted evaluation when combined with batch-oriented search.

For alpha-beta search, prior work has explored GPU parallelization through parallel subtree evaluation~\cite{Rocki2010}. Rocki and Suda demonstrated GPU-accelerated game tree search but found communication overhead limited speedup for complex evaluation functions. Our work extends this analysis to modern unified memory hardware.

\section{Conclusion}

MetalFish provides a complete implementation of GPU-accelerated NNUE evaluation on Apple Silicon, demonstrating both the potential and limitations of GPU acceleration for chess engines. Key findings:

\begin{enumerate}
\item Unified memory successfully eliminates data transfer overhead
\item GPU achieves 44 GB/s shader throughput and 15.7M feature extractions/second
\item Dispatch overhead makes single-position GPU evaluation impractical for alpha-beta
\item Batch sizes of 4--8+ positions are needed for GPU to be competitive
\item The engine achieves 1.4M nodes/second using CPU evaluation
\end{enumerate}

Our implementation provides a tested foundation for future research into batch-oriented GPU chess evaluation, including potential integration with MCTS or speculative evaluation strategies.

\subsection*{Reproducibility}

Hardware: Apple M2 Max, 64GB unified memory. Software: macOS 14.0, Xcode 15.0. Source code: \url{https://github.com/NripeshN/MetalFish}. NNUE networks: nn-c288c895ea92.nnue (125MB), nn-37f18f62d772.nnue (6MB).

\section*{Acknowledgments}

Thanks to the Stockfish and Leela Chess Zero teams for open-source contributions informing this work.

\begin{thebibliography}{10}

\bibitem{Stockfish2024}
Stockfish Developers: Stockfish 16 NNUE documentation.
\url{https://github.com/official-stockfish/Stockfish} (2024)

\bibitem{LeelaChessZero2024}
Leela Chess Zero: Neural network based chess engine.
\url{https://lczero.org/} (2024)

\bibitem{Silver2017}
Silver, D., et al.: Mastering chess and shogi by self-play with a general reinforcement learning algorithm.
arXiv:1712.01815 (2017)

\bibitem{Rocki2010}
Rocki, K., Suda, R.: Parallel minimax tree searching on GPU.
In: Parallel Processing and Applied Mathematics, LNCS vol. 6067, pp. 449--456. Springer (2010)

\bibitem{Knuth1975}
Knuth, D.E., Moore, R.W.: An analysis of alpha-beta pruning.
Artificial Intelligence 6(4), 293--326 (1975)

\bibitem{Nasu2018}
Nasu, Y.: Efficiently updatable neural-network-based evaluation functions for computer shogi.
The 28th World Computer Shogi Championship Appeal Document (2018)

\bibitem{AppleMetal2024}
Apple Inc.: Metal Best Practices Guide.
\url{https://developer.apple.com/metal/} (2024)

\end{thebibliography}

\end{document}
